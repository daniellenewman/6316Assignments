{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Benchmarking Fashion-MNIST with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 6316 Machine Learning - Department of Computer Science - University of Virginia\n",
    "\"The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\" - **Zalando Research, Github Repo.**\"\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "![Here's an example how the data looks (each class takes three-rows):](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "In this assignment, you will attempt to benchmarking the Fashion-MNIST using ANNs. You must use it to train some neural networks on TensorFlow and predict the final output of 10 classes. For deliverables, you must write code in Python and submit this Jupyter Notebook file (.ipynb) to earn a total of 100 pts. You will gain points depending on how you perform in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. PRE-PROCESSING THE DATA (20 pts)\n",
    "\n",
    "You can load the Fashion MNIST directly from Tensorflow using the folliwng code:\n",
    "    tf.keras.datasets.fashion_mnist.load_data();\n",
    "\n",
    "Write some code to load the data file and take a quick look at the dataset, and output the following:\n",
    "- How big is your dataset? (regarding MB)\n",
    "- How many entries does it have?\n",
    "- How many features does it have?\n",
    "- What are some basic statistics you can learn right away about this dataset?\n",
    "\n",
    "**Large-scale Visualization:** Demonstrate that this dataset is indeed a complex and high dimensional worthy of an attempt on TensorFlow. Again, is there any non-linearly separation among the classes? Discover and plot out all features among the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset (MB): 52.40\n",
      "Number of entries: 70000\n",
      "Number of features: 784\n"
     ]
    }
   ],
   "source": [
    "# You might want to use the following package\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Your code goes here for this section.\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Print dataset statistics\n",
    "print(\"Size of dataset (MB): %.2f\" % ((X_train.nbytes + y_train.nbytes + X_test.nbytes + y_test.nbytes) / (2 ** 20)))\n",
    "print(\"Number of entries:\", len(X_train) + len(X_test))\n",
    "print(\"Number of features:\", X_train[0].shape[0] * X_train[0].shape[1])\n",
    "\n",
    "# Scale data down to a fraction between 0 and 1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\",\n",
    "          \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 2. CONSTRUCTION PHASE (25 pts)\n",
    "\n",
    "Define at least three networks with different structures: Ensure the input layer has the right number of inputs. The best structure often is found through a process of trial and error experimentation:\n",
    "- You may start with a fully connected network structure with two hidden layers.\n",
    "- You may try a few activation functions to see if they affect the performance.\n",
    "- You may use various optimizers to tweak the model parameters to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "ann1 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "ann2 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(784, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "ann3 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(784, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(28, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "ann4 = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(100, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(90, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(80, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(70, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(60, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(50, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(40, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(30, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(20, activation=tf.nn.sigmoid),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "models = [ann1, ann2]\n",
    "\n",
    "for model in models:\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"])\n",
    "\n",
    "# ann1.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "#             loss=\"sparse_categorical_crossentropy\",\n",
    "#             metrics=[\"accuracy\"])\n",
    "\n",
    "# ann2.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "#              loss=\"sparse_categorical_crossentropy\",\n",
    "#              metrics=[\"accuracy\"])\n",
    "\n",
    "# ann3.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "#              loss=\"sparse_categorical_crossentropy\",\n",
    "#              metrics=[\"accuracy\"])\n",
    "\n",
    "# ann4.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "#              loss=\"sparse_categorical_crossentropy\",\n",
    "#              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 3. EXECUTION PHASE (30 pts)\n",
    "\n",
    "For each of the three models of your neural networks, open a TensorFlow session, define the number of epochs and size of the training batch (20 pts): For each model, you must compute the performance measures: Confusion Matrix and Class Accuracy.\n",
    "- Which one yields the best performance measure for your dataset?\n",
    "- You must be able to save the trained model and load it from disk to evaluate a test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2851 - acc: 0.8948\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2737 - acc: 0.8987\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2618 - acc: 0.9035\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2526 - acc: 0.9058\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.2427 - acc: 0.9088\n",
      "Model 1 accuracy: 0.8826\n",
      "Model 1 confusion matrix:\n",
      " [[814   3   7  21   6   2 136   0  11   0]\n",
      " [  6 965   0  23   2   0   3   0   1   0]\n",
      " [ 17   2 696  16 145   0 121   1   2   0]\n",
      " [ 18   3   9 916  17   0  32   0   4   1]\n",
      " [  0   2  48  47 830   0  72   0   1   0]\n",
      " [  0   0   0   1   0 965   0  16   0  18]\n",
      " [103   1  49  33  68   0 738   0   8   0]\n",
      " [  0   0   0   0   0  16   0 960   1  23]\n",
      " [  4   0   0   5   2   4   4   3 977   1]\n",
      " [  0   0   0   0   0   5   1  29   0 965]]\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 0.5046 - acc: 0.8182\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.3836 - acc: 0.8610\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.3412 - acc: 0.8757\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.3140 - acc: 0.8840\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 66us/step - loss: 0.2899 - acc: 0.8932\n",
      "Model 1 accuracy: 0.8741\n",
      "Model 1 confusion matrix:\n",
      " [[806   0  12  14   4   0 159   0   5   0]\n",
      " [  4 969   0  18   4   0   5   0   0   0]\n",
      " [ 10   0 742  10 112   0 126   0   0   0]\n",
      " [ 25  10   9 867  34   0  51   0   4   0]\n",
      " [  0   0  86  37 769   0 106   0   2   0]\n",
      " [  0   0   0   1   0 944   0  31   3  21]\n",
      " [ 89   1  69  23  49   0 757   0  12   0]\n",
      " [  0   0   0   0   0  17   0 961   0  22]\n",
      " [  6   0   1   2   5   3   8   3 972   0]\n",
      " [  0   0   0   0   0   6   1  39   0 954]]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "#ann1.fit(X_train, y_train, epochs=5)\n",
    "#ann2.fit(X_train, y_train, epochs=5)\n",
    "#ann3.fit(X_train, y_train, epochs=5)\n",
    "#ann4.fit(X_train, y_train, epochs=5)\n",
    "\n",
    "#test1_loss, test1_acc = ann1.evaluate(X_test, y_test)\n",
    "#test2_loss, test2_acc = ann2.evaluate(X_test, y_test)\n",
    "#test3_loss, test3_acc = ann3.evaluate(X_test, y_test)\n",
    "#test4_loss, test4_acc = ann3.evaluate(X_test, y_test)\n",
    "#print(\"Accuracy:\", test4_acc)\n",
    "\n",
    "# y_pred = ann3.predict_classes(X_test)\n",
    "# confusion_matrix(y_test, y_pred)\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train, epochs=5)\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    print(\"Model\", i, \"accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Model\", i, \"confusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 4. FINETUNING THE NETWORK (20 pts)\n",
    "\n",
    "You may be able to compare the performance of your method agaist other ML methods below:\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com\n",
    "\n",
    "To improve the performance of your ANN, you can use grid search with cross-validation to find the right set of hyperparameters, but that would take a lot of times (days, sometimes weeks). Therefore, you must make some decision of which reasonable values for each hyperparameter, so that you can restrict the search space. Here's a few decision about the network you have to made and justify:\n",
    "\n",
    "- The number of hidden layers. Why did you pick this many?\n",
    "- The number of neurons per hidden layers. Provide some justifiable reasons\n",
    "- Which activation functions need to be used? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 5. OUTLOOK (5 pts)\n",
    "\n",
    "Plan for the outlook of your system: This will lead to the direction of your future project:\n",
    "- Did your neural network outperform other \"traditional ML technique? Why/why not?\n",
    "- Does your model work well for the future? If not, which model should be further investigated?\n",
    "- Do you satisfy with your system? What do you think needed to improve?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "### NEED HELP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you get stuck in any step in the process, you may find some useful information from:\n",
    "\n",
    " * Consult my lectures and/or the textbook\n",
    " * Talk to the TA, they are available and there to help you during [office hour](https://docs.google.com/document/d/15qB84xjaS-uRJmfKmmQuCz38bLMFaoqdbuRLoZEdOYI/edit#heading=h.72k1pvft525n)\n",
    " * Come talk to me or email me <nn4pj@virginia.edu> with subject starting \"CS6316 Assignment 3:...\".\n",
    " * More on the Fashion-MNIST to be found here: https://hanxiao.github.io/2018/09/28/Fashion-MNIST-Year-In-Review/\n",
    "\n",
    "Best of luck and have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
